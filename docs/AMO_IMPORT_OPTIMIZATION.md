# Оптимизация импорта из AMO CRM API

## Текущие ограничения

### AMO API ограничения:
- **Rate limit**: 7 запросов/сек (0.143 сек между запросами)
- **Максимальный limit**: 250 элементов на страницу
- **Максимальный batch**: 50 компаний в одном фильтре
- **Timeout**: 15 секунд на запрос
- **429 ошибки**: экспоненциальная задержка (2, 4, 8 сек)

### Текущие настройки:
- Batch size для компаний: 50
- Batch size для задач: 20
- Batch size для заметок: 200
- Batch size для контактов: 50
- Limit для пагинации: 250

## Предложения по оптимизации

### 1. Увеличение RAM с 5GB до 8GB

**Влияние:**
- ✅ **Прямое влияние**: минимальное (импорт не упирается в память)
- ✅ **Косвенное влияние**: среднее
  - Больше места для кэширования данных в памяти
  - Меньше swap, если используется
  - Больше места для буферов PostgreSQL
  - Возможность увеличить `work_mem` в PostgreSQL

**Рекомендация**: Увеличение RAM полезно, но не критично. Приоритет: **средний**.

### 2. Оптимизация батчинга (высокий приоритет)

#### 2.1. Увеличение batch size для задач
```python
# Текущее: batch_size = 20
# Предложение: batch_size = 50 (максимум для API)
```
**Эффект**: Сокращение количества запросов на 60% для задач.

#### 2.2. Параллельная обработка батчей (если API позволяет)
- Использовать асинхронные запросы с соблюдением rate limit
- Обрабатывать несколько батчей одновременно (с учетом 7 req/sec)

#### 2.3. Оптимизация размера транзакций
```python
# Текущее: одна транзакция на весь batch компаний
# Предложение: разбить на под-транзакции по 10-20 компаний
```
**Эффект**: Меньше блокировок БД, быстрее коммиты.

### 3. Оптимизация запросов к БД (высокий приоритет)

#### 3.1. Bulk операции для CompanyPhone/CompanyEmail
```python
# Текущее: создание по одному
CompanyPhone.objects.create(...)

# Предложение: bulk_create
phones_to_create = []
for p in extra_phones:
    phones_to_create.append(CompanyPhone(...))
CompanyPhone.objects.bulk_create(phones_to_create, ignore_conflicts=True)
```
**Эффект**: Сокращение запросов к БД на 90%+ для телефонов/email.

#### 3.2. Оптимизация проверки дубликатов
```python
# Текущее: проверка по одному
if CompanyPhone.objects.filter(company=comp, value=normalized).exists():

# Предложение: bulk проверка
existing_phones = set(
    CompanyPhone.objects.filter(company=comp)
    .values_list('value', flat=True)
)
# Затем проверка в памяти
```
**Эффект**: 1 запрос вместо N запросов.

#### 3.3. Использование select_for_update только где нужно
- Убрать лишние блокировки строк
- Использовать только для критических секций

### 4. Оптимизация обработки данных (средний приоритет)

#### 4.1. Ленивая загрузка связанных данных
- Загружать контакты/заметки/задачи только для компаний, которые будут импортированы
- Не загружать данные для компаний, которые пропускаются фильтром

#### 4.2. Кэширование метаданных полей
```python
# Кэшировать field_meta между батчами
# Не пересоздавать для каждого батча
```

#### 4.3. Оптимизация парсинга данных
- Использовать более эффективные алгоритмы парсинга
- Кэшировать результаты нормализации телефонов/ИНН

### 5. Оптимизация структуры запросов (средний приоритет)

#### 5.1. Увеличение limit для пагинации
```python
# Текущее: limit=250
# Проверить: можно ли использовать limit=250 (уже максимум)
# Но можно оптимизировать max_pages
```

#### 5.2. Оптимизация фильтрации на стороне API
- Использовать более специфичные фильтры
- Избегать получения лишних данных

#### 5.3. Параллельные запросы (с соблюдением rate limit)
```python
# Использовать asyncio или threading для параллельных запросов
# Но с общим rate limiter (7 req/sec)
```

### 6. Оптимизация PostgreSQL (высокий приоритет)

#### 6.1. Увеличение work_mem
```sql
-- В postgresql.conf или через ALTER SYSTEM
work_mem = 64MB  -- вместо дефолтных 4MB
```
**Эффект**: Быстрее сортировки и группировки при импорте.

#### 6.2. Увеличение shared_buffers
```sql
shared_buffers = 2GB  -- 25% от 8GB RAM
```
**Эффект**: Больше данных в памяти, меньше обращений к диску.

#### 6.3. Оптимизация индексов
- Проверить, что все нужные индексы созданы
- Убедиться, что индексы используются (EXPLAIN ANALYZE)

#### 6.4. Отключение autovacuum во время импорта (временно)
```sql
-- Только для больших импортов!
ALTER TABLE companies SET (autovacuum_enabled = false);
-- После импорта вернуть обратно
```

### 7. Оптимизация логирования (низкий приоритет)

#### 7.1. Уменьшение уровня логирования
```python
# Во время импорта использовать INFO вместо DEBUG
# Или условное логирование (только для первых N компаний)
```

#### 7.2. Батчинг логов
- Собирать логи в память, писать батчами
- Уменьшить количество записей в лог

### 8. Архитектурные улучшения (долгосрочно)

#### 8.1. Фоновая обработка
- Использовать Celery для фонового импорта
- Разбить на задачи: компании → контакты → заметки → задачи

#### 8.2. Инкрементальный импорт
- Импортировать только измененные компании (по updated_at)
- Использовать webhooks для синхронизации

#### 8.3. Кэширование на уровне приложения
- Redis для кэширования метаданных
- Кэширование результатов нормализации

## Приоритизация

### Критичные (быстрая реализация, большой эффект):
1. ✅ Bulk операции для CompanyPhone/CompanyEmail
2. ✅ Bulk проверка дубликатов
3. ✅ Увеличение batch size для задач до 50
4. ✅ Оптимизация транзакций (под-транзакции по 15 компаний)
5. ✅ Настройки подключения к БД (timeout, CONN_MAX_AGE)

### Важные (средний эффект):
5. Увеличение work_mem в PostgreSQL
6. Увеличение shared_buffers
7. Оптимизация структуры запросов к БД

### Желательные (долгосрочно):
8. Параллельная обработка батчей
9. Инкрементальный импорт
10. Кэширование метаданных

## Ожидаемый эффект

### Текущая скорость (примерно):
- 50 компаний: ~2-5 минут (с контактами/заметками/задачами)
- 1000 компаний: ~40-100 минут

### После оптимизаций:
- 50 компаний: ~1-2 минуты (**ускорение в 2-2.5 раза**)
- 1000 компаний: ~20-40 минут (**ускорение в 2-2.5 раза**)

### Влияние RAM:
- Увеличение с 5GB до 8GB даст **+10-15%** к скорости
- Основной эффект от оптимизации кода: **+100-150%**

## Рекомендации

1. **Сначала оптимизировать код** (bulk операции, батчинг) - это даст наибольший эффект
2. **Затем настроить PostgreSQL** (work_mem, shared_buffers)
3. **Потом увеличить RAM** - это даст дополнительный бонус, но не критично
4. **Долгосрочно**: рассмотреть архитектурные улучшения (Celery, инкрементальный импорт)

## Метрики для мониторинга

- Время импорта на батч компаний
- Количество API запросов
- Количество запросов к БД
- Использование памяти
- Использование CPU
- Время выполнения транзакций
